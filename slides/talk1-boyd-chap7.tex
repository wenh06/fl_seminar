\usepackage{empheq}
\usepackage{cancel}

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Boyd\\Distributed ADMM]{Talk 1: Distributed Optimization and Statistical Learning via ADMM (I)}
\date{2021-4-29}

% \institute[北京航空航天大学] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
% {
% 数学科学学院 \\ % Your institution for the title page
% \medskip
% \textit{wenh06@gmail.com} % Your email address
% 北京航空航天大学 \\
% 数学科学学院 \qquad 北京航空航天大学
% }

% \logo{\includegraphics[height=1.5cm]{logo}}
% \logoii{\includegraphics[height=1cm]{logo2}}

% \date{\footnotesize 2021年4月13日} % Date, can be changed to a custom date

\setlength{\belowdisplayskip}{5pt} \setlength{\belowdisplayshortskip}{5pt}
\setlength{\abovedisplayskip}{5pt} \setlength{\abovedisplayshortskip}{5pt}

%------------------------------------------------

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

%------------------------------------------------

\begin{frame}
% \frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%------------------------------------------------

%------------------------------------------------
%	PRESENTATION SLIDES
%------------------------------------------------


% PPT version (read only share link): https://www.kdocs.cn/l/cigmbsd3uAI4


%------------------------------------------------

\section[Recall ADMM]{Recall of basic ADMM}

%------------------------------------------------
% Page 1

\begin{frame}
\frametitle{Recall of basic ADMM}

A general ADMM optimization problem is formulated as
\begin{align*}
    & \text{minimize} \quad f(x) + g(z) \\
    & \text{subject to} \quad Ax + Bz = c
\end{align*}

\pause

The augmented Lagrangian of this problem is given by
$$\mathcal{L}_{\rho}(x,z,y) = f(x) + g(z) + \langle y, Ax+Bz-c \rangle + \dfrac{1}{\rho}\lVert Ax+Bz-c \rVert^2.$$
The iterations are given by
\begin{align*}
    x^{k+1} & = \argmin_{x} \left\{ \mathcal{L}_{\rho}(x,z^k,y^k) \right\} \\
    z^{k+1} & = \argmin_{z} \left\{ \mathcal{L}_{\rho}(x^{k+1},z,y^k) \right\} \\
    y^{k+1} & = y^{k} + \rho (Ax^{k+1}+Bz^{k+1}-c)
\end{align*}

\end{frame}

%------------------------------------------------
% Page 1

\begin{frame}
\frametitle{Convergence of ADMM}

\noindent Under the conditions
\begin{itemize}
    \item $f,g$ closed, proper convex;
    \item $\mathcal{L}_{0}(x,z,y)$ has a saddle point,
\end{itemize}
\vspace{1em}
then as $k\rightarrow\infty$ one has
\begin{itemize}
    \item feasibility: $Ax^k + By^k - c \rightarrow 0$
    \item objective: $f(x^k) + g(y^k) \rightarrow p^*$
    \item dual: $y^k \rightarrow y^*$
\end{itemize}

\end{frame}

%------------------------------------------------

\section[Basic Consensus]{Consensus Problem}

%------------------------------------------------

\begin{frame}
\frametitle{Consensus Problem}

Assume we have global variable $x \in \mathbb{R}^n$ and ``split'' (or distributed) objective function
$$f(x) = \sum\limits_{i=1}^N f_i(x)$$
e.g. $x$ can be (global) model parameters, DNN weights (and biases, etc.), $f_i$ can be the loss function associated with the $i$-th block (``client'') of data. The optimization problem is
\begin{align*}
    & \text{minimize} \quad \sum\limits_{i=1}^N f_i(x)
\end{align*}
    
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Consensus Problem}

Problem: $f$ is NOT block-separable.

\pause
\vspace{0.8em}

Solution: add a common global variable $z \in \mathbb{R}^n$, so that the optimization problem is formulated as (equivalent to)
\begin{align*}
    & \text{minimize} \quad \sum\limits_{i=1}^N f_i(x_i) \\
    & \text{subject to} \quad x_i - z = 0, \quad i=1,\cdots,N
\end{align*}
which is called a {\bfseries consensus problem}.

\pause
\vspace{0.8em}

One has the augmented Lagrangian
$$\mathcal{L}_{\rho}(x_1,\cdots,x_N,z,y) = \sum\limits_{i=1}^N \left[ f_i(x_i) + \langle y_i, x_i-z \rangle + \dfrac{\rho}{2} \lVert x_i-z \rVert^2 \right]$$

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Consensus Problem}

and ADMM iterations
{\scriptsize
\begin{align*}
    \phantom{\leadsto} & \left( x = (x_1^T,\cdots,x_N^T)^T, x^{k+1} = \argmin_x \left\{ \sum\limits_{i=1}^N \left[ f_i(x_i) + \langle y^k_i, x_i-z^k \rangle + \dfrac{\rho}{2} \lVert x_i-z^k \rVert^2 \right] \right\} \right) \\
    \leadsto \quad & x_i^{k+1} = \argmin_{x_i} \left\{ f_i(x_i) + \langle y^k_i, x_i-z^k \rangle + \dfrac{\rho}{2} \lVert x_i-z^k \rVert^2 \right\} \\
    & z^{k+1} = \argmin_{z} \left\{ \sum\limits_{i=1}^N \left[ f_i(x_i^{k+1}) + \langle y^k_i, x_i^{k+1}-z \rangle + \dfrac{\rho}{2} \lVert x_i^{k+1}-z \rVert^2 \right] \right\} \\
    & \phantom{z^{k+1}} = \argmin_{z} \left\{ \dfrac{N\rho}{2} \lVert z \rVert^2 - \langle z, \sum\limits_{i=1}^N (y_i^k+\rho x_i^{k+1}) \rangle + \cdots \right\} \\
    & \phantom{z^{k+1}} = \dfrac{1}{N} \sum\limits_{i=1}^N \left( \dfrac{y_i^{k}}{\rho} + x_i^{k+1} \right) \\
    & y_i^{k+1} = y_i^k + \rho (x_i^{k+1} - z^{k+1})
\end{align*}
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Consensus Problem}

Simplify notations by letting
$$
\begin{cases}
\overline{x}^k := \dfrac{1}{N} \sum\limits_{i=1}^N x_i^k \\
\overline{y}^k := \dfrac{1}{N} \sum\limits_{i=1}^N y_i^k \\
\end{cases}
$$
then one has the following observations
$$\displaystyle z^{k+1} = \dfrac{1}{N\rho} \sum\limits_{i=1}^N y_i^k + \dfrac{1}{N} \sum\limits_{i=1}^N x_i^{k+1} = \dfrac{\overline{y}^k}{\rho} + \overline{x}^{k+1}$$
and
\begin{align*}
    & y^{k+1}_i = y_i^k + \rho(x_i^{k+1}-z^{k+1}) = y_i^k + \rho(x_i^{k+1}-\dfrac{\overline{y}^k}{\rho} - \overline{x}^{k+1}) \\
    \Rightarrow \quad & \overline{y}^{k+1} = \overline{y}^{k} + \rho (\overline{x}_i^{k+1} - \dfrac{\overline{y}^k}{\rho} - \overline{x}^{k+1}) = 0 \\
    \Rightarrow \quad & \overline{z}^{k+1} = \dfrac{0}{\rho} + \overline{x}^{k+1} = \overline{x}^{k+1} \\
\end{align*}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Consensus Problem}

Then one can rewrite the iterations as
\begin{align*}
x^{k+1}_i & = \argmin\limits_{x_i} \left\{ f_i(x_i) + \langle y_i^k, x_i-\overline{x}^k \rangle + \dfrac{\rho}{2}\lVert x_i-\overline{x}^k \rVert^2 \right\} \\
(z^{k+1} & = \overline{x}^{k+1}) \\
y_i^{k+1} & = y_i^k + \rho(x^{k+1}_i-\overline{x}^{k+1})
\end{align*}
This can be further simplified by setting $u_i = \dfrac{y_i}{\rho}$:
\begin{align*}
x^{k+1}_i & = \argmin\limits_{x_i} \left\{ f_i(x_i) + \dfrac{\rho}{2}\lVert x_i-\overline{x}^k + u_i^k \rVert^2 \right\} = \operatorname{prox}_{f_i,\rho}(\overline{x}^k - u_i^k) \\
(z^{k+1} & = \overline{x}^{k+1}) \\
u_i^{k+1} & = u_i^k + (x^{k+1}_i-\overline{x}^{k+1})
\end{align*}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{{\bfseries Statistical Interpretation} for the ADMM iterations for consensus problem}

At iteration $k+1$, assume $x_i$ has prior distribution
$$x_i \sim N(\overline{x}^k-u_i^k, \rho I_n)$$
or equivalently
$$p(x_i) = \det(2\pi\rho I)^{-1/2} \exp\left(-\dfrac{1}{2} \lVert x_i - \overline{x}^k + u_i^k \rVert_{\rho I}^2 \right)$$

\begin{remark}
As stated in \cite{boyd2011distributed}, the bias of the mean of the above normal distribution, which is $-u_i^k$, can be interpreted as the ``price'' of ``client'' i disagreeing with the consensus $\overline{x}^k$ in the previous (the $k$
-th) iteration. As for why the ``price'' is $-u_i^k$, note that the previous scaled dual update $u_i^k$ is augmented by the bias of the $k$-th $x_i$-update, hence the accumulation of the biases of $x_i$-updates.
\end{remark}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{{\bfseries Statistical Interpretation} for the ADMM iterations for consensus problem}

Let
$$f_i(x_i) = \operatorname{NLL}(x_i) = -\log\operatorname{LH}(x_i)$$
be the negative log likelihood function of $x_i$ (w.r.t. the data (or observations) at the $i$-th ``client''). Then the max a posteriori estimates (MAP) of the parameters $x_i$ are
\begin{align*}
    \operatorname{MAP}(x_i) & = \argmax_{x_i} \left\{ p(x_i) \cdot \operatorname{LH}(x_i) \right\} \\
    & = \argmax_{x_i} \left\{ \exp(-f_i(x_i)) \cdot \det(2\pi\rho I)^{-1/2} \right. \\
    & \qquad \left. \cdot \exp\left( -\dfrac{\rho}{2} \lVert x_i - \overline{x}_i^k + u_i^k \rVert^2 \right) \right\} \\
    & = \argmin_{x_i} \left\{ f_i(x_i) + \dfrac{\rho}{2} \lVert x_i - \overline{x}_i^k + u_i^k \rVert^2 \right\} = x_i^{k+1}
\end{align*}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{{\bfseries Statistical Interpretation} for the ADMM iterations for consensus problem}

i.e. the $(k+1)$-th update of $x_i$ are just the MAP of $x_i$ with given prior distribution.

\vspace{1em}

\begin{remark}
Most federated learning optimization algorithms fall into paradigm of this basic consensus problem (with inexact inner minimization loops), including FedAvg \cite{mcmahan2017fed_avg}, FedOpt(FedAdam, FedAdagrad, ...) \cite{reddi2020fed_opt}, etc.
\end{remark}

\end{frame}

%------------------------------------------------

\section[Consensus with Regularization]{Consensus with Regularization}

%------------------------------------------------

\begin{frame}
\frametitle{Consensus with Regularization}

Consider the problem
\begin{align*}
    & \text{minimize} \quad \sum\limits_{i=1}^N f_i(x_i) + \framebox{g(z)} \\
    & \text{subject to} \quad x_i-z = 0, \quad i=1,\cdots,N
\end{align*}
with regularization term $g(z)$ in the objective function.

\vspace{0.8em}

The ADMM iterations:
{\footnotesize
\begin{align*}
x^{k+1}_i & = \argmin\limits_{x_i} \left\{ f_i(x_i) + \langle y_i^k, x_i-z^k \rangle + \dfrac{\rho}{2}\lVert x_i-z^k \rVert^2 \right\} \\
z^{k+1} & = \argmin\limits_{z} \left\{ g(z) + \sum\limits_{i=1}^N \left( \langle y_i^k, x_i^{k+1}-z \rangle + \dfrac{\rho}{2}\lVert x_i^{k+1}-z \rVert^2 \right) \right\} \\
y_i^{k+1} & = y_i^k + \rho (x^{k+1}_i-z^{k+1})
\end{align*}
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Consensus with Regularization}

One similarly has the following reductions by letting $u_i = \frac{y_i}{\rho}$:
{\footnotesize
\begin{align*}
    z^{k+1} & = \argmin_z \left\{ g(z) + \sum\limits_{i=1}^N \left( \dfrac{\rho}{2}\lVert z \rVert^2 - \langle \rho x_i^{k+1}+y_i^k, z \rangle + \cdots \right) \right\} \\
    & = \argmin_z \left\{ g(z) + N \left( \dfrac{\rho}{2}\lVert z \rVert^2 - \langle \rho \overline{x}^{k+1}+\overline{y}^k, z \rangle + \cdots \right) \right\} \\
    & = \argmin_z \left\{ g(z) + \dfrac{N\rho}{2} \lVert z - \overline{x}^{k+1} - \dfrac{\overline{y}^k}{\rho} \rVert^2 \right\} \\
    & = \argmin_z \left\{ g(z) + \dfrac{N\rho}{2} \lVert z - \overline{x}^{k+1} - \overline{u}^k \rVert^2 \right\} \\
    & = \operatorname{prox}_{g,N\rho}(\overline{x}^{k+1} + \overline{u}^k) \\
    x_i^{k+1} & = \argmin_{x_i} \left\{ f_i(x_i) + \dfrac{\rho}{2} \lVert x_i - z^k + u_i^k \rVert^2 \right\} = \operatorname{prox}_{f_i,\rho}(z^k - u_i^k) \\
    u_i^{k+1} & = u_i^k + (x_i^{k+1} - z^{k+1})
\end{align*}
}

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Consensus with Regularization}

\begin{eg}\ 
\begin{itemize}
\item[(1)] $g(z) = \lambda \lVert z \rVert_1, \lambda > 0,$ then
\begin{align*}
    z^{k+1} & = \argmin_z \left\{ \lambda \lVert z \rVert_1 + \dfrac{N\rho}{2} \lVert z - \overline{x}^{k+1} - \overline{u}^k \rVert^2 \right\} \\
    & = \operatorname{S}_{\lambda/N\rho} (\overline{x}^{k+1} + \overline{u}^{k+1}) \quad \leftarrow \quad \textbf{soft thresholding}
\end{align*}
\item[(2)] $g(z) = I_{\mathbb{R}^n_+}(z)$ the indicator function of $\mathbb{R}^n_+$, then
\begin{align*}
    z^{k+1} & = \argmin_z \left\{ I_{\mathbb{R}^n_+}(z) + \dfrac{N\rho}{2} \lVert z - \overline{x}^{k+1} - \overline{u}^k \rVert^2 \right\} \\
    & = (\overline{x}^{k+1} + \overline{u}^{k+1})_+
\end{align*}
\end{itemize}
\end{eg}
    
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Consensus with Regularization}
    
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Consensus with Regularization}
    
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Consensus with Regularization}
    
\end{frame}

%------------------------------------------------

\section[General Consensus]{General Form Consensus}

%------------------------------------------------

\begin{frame}
\frametitle{General Form Consensus}
    
\end{frame}

%------------------------------------------------

\section[General Consensus with Regularization]{General Form Consensus with Regularization}

%------------------------------------------------

\begin{frame}
\frametitle{General Form Consensus with Regularization}
    
\end{frame}

%------------------------------------------------

\section[Sharing Problem]{Sharing Problem}

%------------------------------------------------

\begin{frame}
\frametitle{Sharing Problem}

\end{frame}

%------------------------------------------------

\begin{frame}[allowframebreaks]
\frametitle{References}

{\footnotesize
\bibliographystyle{ieeetr}
\bibliography{references}
}

\end{frame}

%------------------------------------------------

% \begin{frame}

% \Huge{\centerline{\bfseries The End}}

% \vspace{0.5em}

% \Huge{\centerline{\phantom{a}\bfseries 谢谢！}}

% \end{frame}

%------------------------------------------------

\end{document}
